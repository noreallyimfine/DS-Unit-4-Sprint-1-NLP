{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset/challenge). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more managable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "*Successfully complete these all these objectives to earn a 2. There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "yelp = pd.read_json('./data/review_sample.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(yelp.shape)\n",
    "\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spacy and the tokenizer\n",
    "# maybe will have to switch to md or sm if it causes my comp problems\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the tokenizer \n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) to tokenize a single doc\n",
    "def tokenize(doc):\n",
    "    return [token.text for token in tokenizer(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column of tokenized text\n",
    "yelp['tokens'] = [tokenize(doc) for doc in yelp['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [BEWARE!!!, FAKE,, FAKE,, FAKE....We, also, ow...\n",
       "1    [Came, here, for, lunch, Togo., Service, was, ...\n",
       "2    [I've, been, to, Vegas, dozens, of, times, and...\n",
       "3    [We, went, here, on, a, night, where, they, cl...\n",
       "4    [3.5, to, 4, stars, \\n\\n, Not, bad, for, the, ...\n",
       "5    [Tasty,, fast, casual, Latin, street, food.,  ...\n",
       "6    [This, show, is, absolutely, amazing!!, What, ...\n",
       "7    [Came, for, the, Pho, and, really, enjoyed, it...\n",
       "8    [Absolutely, the, most, Unique, experience, in...\n",
       "9    [Wow., I, walked, in, and, sat, at, the, bar, ...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and it seems to have worked\n",
    "yelp['tokens'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews\n",
    "2. Write a fake review and query for the 10 most similiar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, it will probably be best to use a `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join back the token lists into strings\n",
    "data =  [' '.join(s) for s in yelp['tokens']]\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>amazing</th>\n",
       "      <th>area</th>\n",
       "      <th>asked</th>\n",
       "      <th>away</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bad</th>\n",
       "      <th>bar</th>\n",
       "      <th>best</th>\n",
       "      <th>better</th>\n",
       "      <th>...</th>\n",
       "      <th>vegas</th>\n",
       "      <th>wait</th>\n",
       "      <th>want</th>\n",
       "      <th>wanted</th>\n",
       "      <th>wasn</th>\n",
       "      <th>way</th>\n",
       "      <th>went</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.381265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23316</td>\n",
       "      <td>0.466138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10  amazing     area  asked  away  awesome      bad       bar      best  \\\n",
       "0  0.0      0.0  0.00000    0.0   0.0      0.0  0.00000  0.000000  0.000000   \n",
       "1  0.0      0.0  0.00000    0.0   0.0      0.0  0.00000  0.000000  0.000000   \n",
       "2  0.0      0.0  0.14922    0.0   0.0      0.0  0.00000  0.000000  0.000000   \n",
       "3  0.0      0.0  0.00000    0.0   0.0      0.0  0.00000  0.000000  0.360664   \n",
       "4  0.0      0.0  0.00000    0.0   0.0      0.0  0.23316  0.466138  0.000000   \n",
       "\n",
       "   better  ...     vegas      wait  want  wanted  wasn  way      went  work  \\\n",
       "0     0.0  ...  0.000000  0.392384   0.0     0.0   0.0  0.0  0.000000   0.0   \n",
       "1     0.0  ...  0.000000  0.000000   0.0     0.0   0.0  0.0  0.000000   0.0   \n",
       "2     0.0  ...  0.292434  0.000000   0.0     0.0   0.0  0.0  0.123145   0.0   \n",
       "3     0.0  ...  0.000000  0.000000   0.0     0.0   0.0  0.0  0.381265   0.0   \n",
       "4     0.0  ...  0.000000  0.000000   0.0     0.0   0.0  0.0  0.000000   0.0   \n",
       "\n",
       "   worth  years  \n",
       "0    0.0    0.0  \n",
       "1    0.0    0.0  \n",
       "2    0.0    0.0  \n",
       "3    0.0    0.0  \n",
       "4    0.0    0.0  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the standard english stop words, and cutting out 5% outliers\n",
    "vect = TfidfVectorizer(stop_words='english', min_df=0.05, max_df=0.95)\n",
    "\n",
    "# fit to the data\n",
    "vect.fit(data)\n",
    "\n",
    "# vectorize data into sparse matrix\n",
    "sparse = vect.transform(data)\n",
    "\n",
    "# create a df of the matrix but filled in with zeros\n",
    "dtm = pd.DataFrame(sparse.todense(), columns=vect.get_feature_names())\n",
    "\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "         metric_params=None, n_jobs=None, n_neighbors=5, p=2, radius=1.0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the model very simply\n",
    "nn = NearestNeighbors(n_neighbors=5, algorithm='ball_tree')\n",
    "\n",
    "# fit to dataframe from vectorized data\n",
    "nn.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake review to test model\n",
    "fake_review = ['''\n",
    "    This product is terrible. It broke the first time I used it.\n",
    "    I want my money back\n",
    "''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.60053305, 0.60053305, 0.60053305, 0.82027879]]),\n",
       " array([[9037, 9181, 5547, 8411, 5518]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize review\n",
    "search = vect.transform(fake_review)\n",
    "\n",
    "# run the search\n",
    "nn.kneighbors(search.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model not great but these reviews all do have strongly negative sentiments so it nailed that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you want to be seen by a DR. who looks at his laptop more than he looks at you or spend time with you or even listen to you, this is the one to go to!'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[9037]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The man that answers the phone has a horrible attitude. If they do NOT want to do Delivary shouldn't be advertising as so.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[8411]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over priced hipster jive ass baking. The chocolate babka is getting them the extra star but you can pass on the rest unless you want some greasy croissants or some other stomach ache inducing nonsense. Keep on walking. Nothing to see here.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[5547]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a piepline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier. Use that pipeline to estimate a model to predict `stars`. Use the Pipeline to predict a star rating for your fake review from Part 2. \n",
    "2. Tune the entire pipeline with a GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_iter=7)\n",
    "vect = TfidfVectorizer(tokenizer=tokenize)\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "lsi = Pipeline([('vect', vect), ('svd', svd)])\n",
    "pipe = Pipeline([('lsi', lsi), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'lsi__vect__max_df': [0.925, 0.95, 1.0],\n",
    "    'lsi__vect__ngram_range': [(1,1), (1,2)],\n",
    "    'lsi__svd__n_components': [2,5,10],\n",
    "    'clf__n_estimators': [50, 100, 200],\n",
    "    'clf__max_depth': [3,4,5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1727s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0090s.) Setting batch_size=88.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/queues.py\", line 150, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\", line 243, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/reduction.py\", line 236, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py\", line 284, in dump\n    return Pickler.dump(self, obj)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 437, in dump\n    self.save(obj)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 887, in _batch_setitems\n    save(v)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 816, in save_list\n    self._batch_appends(obj)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 843, in _batch_appends\n    save(tmp[0])\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 771, in save_tuple\n    save(element)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 771, in save_tuple\n    save(element)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 816, in save_list\n    self._batch_appends(obj)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 840, in _batch_appends\n    save(x)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 771, in save_tuple\n    save(element)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 816, in save_list\n    self._batch_appends(obj)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 840, in _batch_appends\n    save(x)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 771, in save_tuple\n    save(element)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py\", line 414, in save_function\n    self.save_function_tuple(obj)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py\", line 579, in save_function_tuple\n    save(state)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 882, in _batch_setitems\n    save(v)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 856, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 887, in _batch_setitems\n    save(v)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 638, in save_reduce\n    save(args)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 786, in save_tuple\n    save(element)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 786, in save_tuple\n    save(element)\n  File \"/home/jay/anaconda3/envs/nlp-u4-s1/lib/python3.7/pickle.py\", line 524, in save\n    rv = reduce(self.proto)\n  File \"stringsource\", line 2, in preshed.maps.PreshMap.__reduce_cython__\nTypeError: self.c_map cannot be converted to a Python object for pickling\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-12aa91f24b4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-u4-s1/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-u4-s1/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp-u4-s1/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "X = yelp['text']\n",
    "y = yelp['stars']\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=3, n_jobs=4, verbose=10)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       4\n",
       "2       3\n",
       "3       1\n",
       "4       4\n",
       "5       4\n",
       "6       5\n",
       "7       4\n",
       "8       5\n",
       "9       1\n",
       "10      4\n",
       "11      1\n",
       "12      5\n",
       "13      4\n",
       "14      1\n",
       "15      4\n",
       "16      5\n",
       "17      5\n",
       "18      1\n",
       "19      5\n",
       "20      5\n",
       "21      5\n",
       "22      2\n",
       "23      4\n",
       "24      2\n",
       "25      5\n",
       "26      4\n",
       "27      5\n",
       "28      5\n",
       "29      5\n",
       "       ..\n",
       "9970    5\n",
       "9971    5\n",
       "9972    5\n",
       "9973    2\n",
       "9974    5\n",
       "9975    5\n",
       "9976    5\n",
       "9977    1\n",
       "9978    5\n",
       "9979    1\n",
       "9980    1\n",
       "9981    5\n",
       "9982    5\n",
       "9983    5\n",
       "9984    5\n",
       "9985    5\n",
       "9986    1\n",
       "9987    5\n",
       "9988    5\n",
       "9989    5\n",
       "9990    5\n",
       "9991    5\n",
       "9992    5\n",
       "9993    1\n",
       "9994    4\n",
       "9995    1\n",
       "9996    3\n",
       "9997    2\n",
       "9998    4\n",
       "9999    1\n",
       "Name: stars, Length: 10000, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp['stars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "    - Keep the `iterations` parameter at or below 5 to reduce run time\n",
    "    - The `workers` parameter should match the number of physical cores on your machine.\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the vocubalary of the yelp data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words representation of the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "corpus = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your LDA model should be ready for estimation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(corpus=corpus,\n",
    "                   id2word=id2word,\n",
    "                   iterations=5,\n",
    "                   workers=4,\n",
    "                   num_topics = 10 # You can change this parameter\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1-2 visualizations of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goals\n",
    "\n",
    "Complete one of more of these to push your score towards a three: \n",
    "* Incorporate named entity recognition into your analysis\n",
    "* Compare vectorization methods in the classification section\n",
    "* Analyze more (or all) of the yelp dataset - this one is v. hard. \n",
    "* Use a generator object on the reviews file - this would help you with the analyzing the whole dataset.\n",
    "* Incorporate any of the other yelp dataset entities in your analysis (business, users, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "nlp-u4-s1 (Python3)",
   "language": "python",
   "name": "nlp-u4-s1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
